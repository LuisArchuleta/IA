{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3J91v2YI9Mg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. CARGA Y PREPROCESAMIENTO DE DATOS\n",
        "# ==============================================================================\n",
        "# Cargamos el famoso dataset MNIST (dígitos escritos a mano)\n",
        "print(\"Cargando y preprocesando datos MNIST...\")\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalizamos las imágenes a un rango de 0 a 1 (Escalamiento)\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# El MLP espera datos planos (flat). Convertimos las imágenes de 28x28 a un vector de 784.\n",
        "# (Esta es la capa de entrada del Perceptrón)\n",
        "x_train_flat = x_train.reshape(-1, 28 * 28)\n",
        "x_test_flat = x_test.reshape(-1, 28 * 28)\n",
        "\n",
        "print(f\"Forma de los datos de entrada (plano): {x_train_flat.shape}\")\n",
        "\n",
        "# 2. DEFINICIÓN DEL MODELO (MLP - Multi-Layer Perceptron)\n",
        "# ==============================================================================\n",
        "# Un MLP es una secuencia de capas densas (perceptrones)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # Capa de Entrada: 784 neuronas (28x28 píxeles)\n",
        "    keras.layers.InputLayer(input_shape=(784,)),\n",
        "\n",
        "    # Capa Oculta 1 (Perceptrones): 128 neuronas. Usa ReLU como función de activación.\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "\n",
        "    # Capa Oculta 2 (Perceptrones): 64 neuronas.\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "\n",
        "    # Capa de Salida: 10 neuronas (una para cada dígito, 0-9). Usa Softmax para probabilidades.\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 3. COMPILACIÓN DEL MODELO\n",
        "# ==============================================================================\n",
        "# La elección del optimizador (Adam) define cómo se ejecutará el algoritmo de Backpropagation.\n",
        "# Adam es un optimizador basado en Descenso de Gradiente, y usa Backpropagation para\n",
        "# calcular los gradientes y ajustar los pesos.\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy', # Pérdida para clasificación multi-clase\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Imprimir el resumen del modelo (opcional)\n",
        "print(\"\\n--- Estructura del MLP ---\")\n",
        "model.summary()\n",
        "\n",
        "# 4. ENTRENAMIENTO (EJECUCIÓN DEL BACKPROPAGATION)\n",
        "# ==============================================================================\n",
        "print(\"\\nIniciando entrenamiento (Backpropagation)...\")\n",
        "history = model.fit(\n",
        "    x_train_flat,\n",
        "    y_train,\n",
        "    epochs=10, # Número de pasadas completas sobre los datos\n",
        "    validation_split=0.15, # Usar 15% de datos para validación\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5. EVALUACIÓN Y RESULTADOS\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Evaluación Final ---\")\n",
        "test_loss, test_acc = model.evaluate(x_test_flat, y_test, verbose=0)\n",
        "\n",
        "print(f\"Pérdida en Datos de Prueba: {test_loss:.4f}\")\n",
        "print(f\"Precisión en Datos de Prueba: {test_acc*100:.2f}%\")\n",
        "\n",
        "# Gráfico de la pérdida durante el entrenamiento\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n",
        "plt.title('Pérdida del Modelo durante Backpropagation')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ]
}